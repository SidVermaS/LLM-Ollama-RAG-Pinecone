{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f860f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3d9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "INDEX_NAME=\"demo-index\"\n",
    "if INDEX_NAME not in pc.list_indexes().names():\n",
    "    pc.create_index(name=INDEX_NAME, dimension=768, metric=\"cosine\", spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0216a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ollama:11434\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings \n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\")\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=OLLAMA_HOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bed3bf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index host ignored when initializing with index object.\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c530b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"technical.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28a89425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, separators = [\"\\n\\n\",\"\\n\",\" \"])\n",
    "splitted_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba5fa5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.add_documents(splitted_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "060408dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "    You are an AI assitant.\n",
    "    If the answer isn't in the context, say you don't know\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template = template, input_variables = [\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b067a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model = \"llama3.2:1b\", temperature = 0.5, base_url = OLLAMA_HOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d220352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    chain_type_kwargs = {\n",
    "        \"prompt\": prompt\n",
    "    },\n",
    "    retriever = vector_store.as_retriever(search_kwargs = { \"k\": 2 })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ffef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's the plan?\"\n",
    "\n",
    "result = qa_chain(query)\n",
    "# result = qa_chain.invoke({ \"query\": query })\n",
    "print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
